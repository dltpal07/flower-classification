{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "flower_classification_train",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "1TsuMP9FrX1T",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "outputId": "770c951b-2af4-42f7-9861-bceb5d050020"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GnX4aY6ha4mR",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 93
        },
        "outputId": "9b84ff00-1e28-4168-a68f-3144bd373afd"
      },
      "source": [
        "import numpy as np\n",
        "import random\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import cv2\n",
        "import os\n",
        "import pandas as pd\n",
        "from random import shuffle\n",
        "from tqdm import tqdm\n",
        "img_size_flat=16384\n",
        "n_t_xy=np.load('/content/gdrive/My Drive/Colab Notebooks/flower_recognition/AIDD/aidd_flower.npy',allow_pickle=True)\n",
        "s_t_xy=np.load('/content/gdrive/My Drive/Colab Notebooks/flower_recognition/AIDD/aidd_flower.npy',allow_pickle=True)\n",
        "print(n_t_xy.shape)\n",
        "n_xy=np.load('/content/gdrive/My Drive/Colab Notebooks/flower_recognition/AIDD/aidd_flower.npy',allow_pickle=True)\n",
        "print(n_xy.shape)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(980, 2)\n",
            "(980, 2)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nclass_test=s_t_xy[10:30]\\nc_test_x = np.array([i[0] for i in class_test])\\nc_test_x=c_test_x.reshape(-1,img_size_flat)\\nc_test_y = np.array([i[1] for i in class_test])\\n\\nclass_test1=s_t_xy[260:280]\\nc_test_x1 = np.array([i[0] for i in class_test1])\\nc_test_x1=c_test_x1.reshape(-1,img_size_flat)\\nc_test_y1 = np.array([i[1] for i in class_test1])\\n\\nclass_test2=s_t_xy[600:660]\\nc_test_x2 = np.array([i[0] for i in class_test2])\\nc_test_x2=c_test_x2.reshape(-1,img_size_flat)\\nc_test_y2 = np.array([i[1] for i in class_test2])\\n\\nclass_test3=s_t_xy[800:830]\\nc_test_x3 = np.array([i[0] for i in class_test3])\\nc_test_x3=c_test_x3.reshape(-1,img_size_flat)\\nc_test_y3 = np.array([i[1] for i in class_test3])\\n\\nclass_test4=s_t_xy[1020:1040]\\nc_test_x4 = np.array([i[0] for i in class_test4])\\nc_test_x4=c_test_x4.reshape(-1,img_size_flat)\\nc_test_y4 = np.array([i[1] for i in class_test4])\\n\\nclass_test5=s_t_xy[1260:1300]\\nc_test_x5 = np.array([i[0] for i in class_test5])\\nc_test_x5=c_test_x5.reshape(-1,img_size_flat)\\nc_test_y5 = np.array([i[1] for i in class_test5])\\n\\nclass_test6=s_t_xy[1520:1570]\\nc_test_x6 = np.array([i[0] for i in class_test6])\\nc_test_x6=c_test_x6.reshape(-1,img_size_flat)\\nc_test_y6 = np.array([i[1] for i in class_test6])\\n\\n'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4DGCSN4Lrc_M",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 594
        },
        "outputId": "80bf7b00-fc8a-47a0-82e0-eb7eb0d6cd6b"
      },
      "source": [
        "import numpy as np\n",
        "import random\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import cv2\n",
        "import os\n",
        "import pandas as pd\n",
        "from random import shuffle\n",
        "from tqdm import tqdm\n",
        "\n",
        "#t=np.loadtxt('/content/gdrive/My Drive/Colab Notebooks/flower/tttt.npy')\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "# Convolutional Layer 1.\n",
        "filter_size1 = 4 \n",
        "num_filters1 = 32\n",
        "# Convolutional Layer 2.\n",
        "filter_size2 = 4\n",
        "num_filters2 = 64\n",
        "# Convolutional Layer 3.\n",
        "filter_size3 = 4\n",
        "num_filters3 = 128\n",
        "# Convolutional Layer 4\n",
        "filter_size4 = 4\n",
        "num_filters4 = 256\n",
        "# Convolutional Layer 5\n",
        "filter_size5 = 4\n",
        "num_filters5 = 128\n",
        "# Fully-connected layer.\n",
        "fc_size = 1024         \n",
        "# Number of color channels for the images: 1 channel for gray-scale.  ########################우리는 흑백이랑 컬러 2개...?\n",
        "num_channels = 1\n",
        "# image dimensions (only squares for now)   #################\n",
        "img_size = 128\n",
        "# Size of image when flattened to a single dimension\n",
        "img_size_flat = img_size * img_size * num_channels\n",
        "# Tuple with height and width of images used to reshape arrays.\n",
        "img_shape = (img_size, img_size)\n",
        "# class 써줌\n",
        "classes = ['poppy',  'sunflower', 'lily','roseofsharon','tulip','rose','korearose','lotus']\n",
        "num_classes = len(classes)\n",
        "# batch size\n",
        "batch_size = 50\n",
        "keep_prob=tf.placeholder(tf.float32,name='keep_prob')\n",
        "\n",
        "\n",
        "#data 불러옴\n",
        "checkpoint_dir = \"models/\"\n",
        "\n",
        "#train_data = np.load('flower_train_data.npy')\n",
        "\n",
        "def new_weights(shape):\n",
        "    return tf.Variable(tf.truncated_normal(shape, stddev=0.05))\n",
        "def new_biases(length):\n",
        "    return tf.Variable(tf.constant(0.05, shape=[length]))\n",
        "\n",
        "def new_conv_layer(input,num_input_channels,filter_size,num_filters,use_pooling=True):  \n",
        "\n",
        "    shape = [filter_size, filter_size, num_input_channels, num_filters]\n",
        "    weights = new_weights(shape=shape)\n",
        "    biases = new_biases(length=num_filters)\n",
        "\n",
        "    layer = tf.nn.conv2d(input=input,filter=weights,strides=[1, 1, 1, 1],padding='SAME')\n",
        "    layer += biases\n",
        "\n",
        "    if use_pooling:\n",
        "        layer = tf.nn.max_pool(value=layer,ksize=[1, 2, 2, 1],strides=[1, 2, 2, 1],padding='SAME')\n",
        "\n",
        "    layer = tf.nn.relu(layer)\n",
        "    \n",
        "    return layer, weights\n",
        "\n",
        "def new_fc_layer(input,num_inputs,num_outputs,use_relu=True): \n",
        "\n",
        "    weights = new_weights(shape=[num_inputs, num_outputs])\n",
        "    biases = new_biases(length=num_outputs)\n",
        "\n",
        "    layer = tf.matmul(input, weights) + biases\n",
        "    \n",
        "    if use_relu:\n",
        "        layer = tf.nn.relu(layer)\n",
        "\n",
        "    return layer\n",
        "\n",
        "def flatten_layer(layer):\n",
        "    \n",
        "    layer_shape = layer.get_shape()\n",
        "    num_features = layer_shape[1:4].num_elements()\n",
        "    layer_flat = tf.reshape(layer, [-1, num_features])\n",
        "\n",
        "    return layer_flat, num_features\n",
        "\n",
        "x = tf.placeholder(tf.float32, shape=[None, img_size_flat], name='x')\n",
        "x_image = tf.reshape(x, [-1, img_size, img_size, num_channels])\n",
        "y_true = tf.placeholder(tf.float32, shape=[None, num_classes], name='y_true')\n",
        "y_true_cls = tf.argmax(y_true, axis=1)\n",
        "\n",
        "layer_conv1, weights_conv1 = new_conv_layer(input=x_image,num_input_channels=num_channels,filter_size=filter_size1,\n",
        "                                            num_filters=num_filters1,use_pooling=True)\n",
        "layer_conv1=tf.nn.dropout(layer_conv1,keep_prob=keep_prob)\n",
        "layer_conv2, weights_conv2 = new_conv_layer(input=layer_conv1,num_input_channels=num_filters1,filter_size=filter_size2,\n",
        "                                            num_filters=num_filters2,use_pooling=True)\n",
        "layer_conv2=tf.nn.dropout(layer_conv2,keep_prob=keep_prob)\n",
        "layer_conv3, weights_conv3 = new_conv_layer(input=layer_conv2,num_input_channels=num_filters2,filter_size=filter_size3,\n",
        "                                            num_filters=num_filters3,use_pooling=True)\n",
        "layer_conv3=tf.nn.dropout(layer_conv3,keep_prob=keep_prob)\n",
        "layer_conv4, weights_conv4 = new_conv_layer(input=layer_conv3,num_input_channels=num_filters3,filter_size=filter_size4,\n",
        "                                            num_filters=num_filters4,use_pooling=True)\n",
        "layer_conv5=tf.nn.dropout(layer_conv4,keep_prob=keep_prob)\n",
        "layer_conv5, weights_conv5 = new_conv_layer(input=layer_conv4,num_input_channels=num_filters4,filter_size=filter_size5,\n",
        "                                            num_filters=num_filters5,use_pooling=True)\n",
        "layer_conv5=tf.nn.dropout(layer_conv5,keep_prob=keep_prob)\n",
        "layer_flat, num_features = flatten_layer(layer_conv5)\n",
        "layer_fc1 = new_fc_layer(input=layer_flat,num_inputs=num_features,num_outputs=fc_size,use_relu=True)\n",
        "layer_fc2 = new_fc_layer(input=layer_fc1,num_inputs=fc_size,num_outputs=num_classes,use_relu=False)\n",
        "\n",
        "y_pred = tf.nn.softmax(layer_fc2,name='operation')\n",
        "y_pred_cls = tf.argmax(y_pred, axis=1)\n",
        "cross_entropy = tf.nn.softmax_cross_entropy_with_logits(logits=layer_fc2,labels=y_true)\n",
        "cost = tf.reduce_mean(cross_entropy)\n",
        "\n",
        "optimizer = tf.train.AdamOptimizer(learning_rate=1e-4).minimize(cost)\n",
        "correct_prediction = tf.equal(y_pred_cls, y_true_cls)\n",
        "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
        "\n",
        "\n",
        "\n",
        "shuffle(n_xy)\n",
        "train=n_xy\n",
        "\n",
        "test=n_t_xy\n",
        "\n",
        "shuffle(test)\n",
        "\n",
        "print(train.shape,test.shape)\n",
        "\n",
        "x_batch = np.array([i[0] for i in train])\n",
        "x_batch=x_batch.reshape(-1,img_size_flat)\n",
        "#x_batch = np.array([i[0] for i in train])\n",
        "\n",
        "y_true_batch = np.array([i[1] for i in train])\n",
        "\n",
        "test_x = np.array([i[0] for i in test])\n",
        "test_x=test_x.reshape(-1,img_size_flat)\n",
        "#test_x = np.array([i[0] for i in test])\n",
        "\n",
        "test_y = np.array([i[1] for i in test])\n",
        "\n",
        "\n",
        "print(len(test_x),len(test_y))\n",
        "#for i in range(100):\n",
        "#  plt.imshow((c_test_x9[i]).reshape(28,28))\n",
        "#  plt.show()\n",
        "with tf.Session() as session:    \n",
        "    global total_iterations\n",
        "    init_op = tf.initialize_all_variables()\n",
        "    session.run(init_op)\n",
        "    session.run(tf.global_variables_initializer())\n",
        "    total_iterations = 0\n",
        "    num_iterations=200\n",
        "    for i in range(total_iterations,total_iterations + num_iterations):\n",
        "      \n",
        "      a = 0\n",
        "      for __ in range(int(len(train)/batch_size)):\n",
        "        \n",
        "        \n",
        "          feed_dict_train = {x: x_batch[a:a+batch_size,:],y_true: y_true_batch[a:a+batch_size],keep_prob:1.0}\n",
        "          session.run(optimizer, feed_dict=feed_dict_train)\n",
        "          a = a + batch_size\n",
        "      #plt.imshow(x_batch[i : i + 1].reshape(28, 28))\n",
        "      #plt.show()\n",
        "      if i % 2 == 0:\n",
        "         print(\"Iteration = \", i, \"Loss = \", session.run(cost, feed_dict=feed_dict_train),\n",
        "                      \"Train Accuracy = \", session.run(accuracy, feed_dict=feed_dict_train), \n",
        "                      \"Test Accuracy = \", session.run(accuracy, feed_dict={x:test_x, y_true: test_y,keep_prob:1.0}))\n",
        "\n",
        "\n",
        "   \n",
        "       \n",
        "\n",
        "#optimize(num_iterations=100)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING: Logging before flag parsing goes to stderr.\n",
            "W0708 10:53:09.415184 140529386407808 deprecation.py:506] From <ipython-input-2-e7c84b0f4f72>:100: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
            "W0708 10:53:09.590442 140529386407808 deprecation.py:323] From <ipython-input-2-e7c84b0f4f72>:119: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "\n",
            "Future major versions of TensorFlow will allow gradients to flow\n",
            "into the labels input on backprop by default.\n",
            "\n",
            "See `tf.nn.softmax_cross_entropy_with_logits_v2`.\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "(980, 2) (980, 2)\n",
            "980 "
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "W0708 10:53:11.351383 140529386407808 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/util/tf_should_use.py:193: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
            "Instructions for updating:\n",
            "Use `tf.global_variables_initializer` instead.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "980\n",
            "Iteration =  0 Loss =  25.501072 Train Accuracy =  0.14 Test Accuracy =  0.06530612\n",
            "Iteration =  2 Loss =  6.023158 Train Accuracy =  0.38 Test Accuracy =  0.27857143\n",
            "Iteration =  4 Loss =  2.4063172 Train Accuracy =  0.56 Test Accuracy =  0.6010204\n",
            "Iteration =  6 Loss =  0.8141873 Train Accuracy =  0.82 Test Accuracy =  0.6704082\n",
            "Iteration =  8 Loss =  0.5366695 Train Accuracy =  0.9 Test Accuracy =  0.83265305\n",
            "Iteration =  10 Loss =  0.34566867 Train Accuracy =  0.92 Test Accuracy =  0.85918367\n",
            "Iteration =  12 Loss =  0.16597402 Train Accuracy =  0.96 Test Accuracy =  0.8642857\n",
            "Iteration =  14 Loss =  0.09603994 Train Accuracy =  0.98 Test Accuracy =  0.8683674\n",
            "Iteration =  16 Loss =  0.055714283 Train Accuracy =  1.0 Test Accuracy =  0.8704082\n",
            "Iteration =  18 Loss =  0.035027843 Train Accuracy =  1.0 Test Accuracy =  0.86938775\n",
            "Iteration =  20 Loss =  0.02551163 Train Accuracy =  1.0 Test Accuracy =  0.87142855\n",
            "Iteration =  22 Loss =  0.020048799 Train Accuracy =  1.0 Test Accuracy =  0.87142855\n",
            "Iteration =  24 Loss =  0.016518028 Train Accuracy =  1.0 Test Accuracy =  0.87142855\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}